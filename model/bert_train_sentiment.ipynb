{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import os\n",
    "import yaml\n",
    "import wandb\n",
    "import torch\n",
    "from torch import cuda\n",
    "from datetime import datetime\n",
    "from utils.data_utils import read_data, filter_fallacies, encode_labels_sentiment, plot_training_curve, \\\n",
    "    plot_learning_curve\n",
    "from model.BERT import compute_metrics_wandb, DataLoader\n",
    "from transformers import BertTokenizer, TrainingArguments, Trainer, EarlyStoppingCallback"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Load configuration file\n",
    "with open('config.yaml', 'r') as file:\n",
    "    config = yaml.safe_load(file)\n",
    "\n",
    "    # Start a new wandb run to track this script\n",
    "run = wandb.init(\n",
    "    project=\"Logical_Fallacies\",\n",
    "    config=config,\n",
    "    allow_val_change=True,  # Allows you to update the config during the run\n",
    "    settings=wandb.Settings(console=\"off\")\n",
    ")\n",
    "current_time = datetime.now()"
   ],
   "id": "178a88cfe25369fc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "train_dataset, test_dataset, dev_dataset = read_data('combined_lfud_huggingface_nonfallacies_sent.csv')\n",
    "dev_dataset.head()"
   ],
   "id": "91f9ead4aac5b742",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "dev_dataset['logical_fallacies'].value_counts()",
   "id": "297c8fbfcf225d58",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "train_dataset['logical_fallacies'].value_counts()",
   "id": "597c36a02af5d561",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "test_dataset['logical_fallacies'].value_counts()",
   "id": "6988f319809bcdc7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "logical_fallacies = list(set(list(train_dataset['logical_fallacies'])))\n",
    "logical_fallacies"
   ],
   "id": "32489efc6fbb30ff",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# logical_fallacies_subset = ['nonfallacy', 'faulty generalization', 'intentional']\n",
    "# logical_fallacies_subset = ['nonfallacy', 'faulty generalization', 'intentional', 'ad hominem', 'false causality']\n",
    "logical_fallacies_subset = ['faulty generalization', 'false dilemma', 'appeal to emotion',\n",
    "                            'deductive fallacy', 'fallacy of extension', 'false causality', 'fallacy of relevance',\n",
    "                            'intentional', 'ad hominem', 'circular reasoning', 'fallacy of credibility',\n",
    "                            'ad populum', 'equivocation', 'nonfallacy', 'fallacy of logic']\n",
    "\n",
    "fil_train_data, fil_test_data, fil_dev_data = filter_fallacies(train_dataset, test_dataset, dev_dataset,\n",
    "                                                               logical_fallacies_subset)\n",
    "\n",
    "fil_train_data.head()"
   ],
   "id": "17ac33e20197dcf",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "logical_fallacies_subset",
   "id": "7bac7006d196460b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "id2label = {id: label for id, label in enumerate(logical_fallacies_subset)}\n",
    "\n",
    "label2id = {label: id for id, label in enumerate(logical_fallacies_subset)}\n",
    "\n",
    "print(label2id, id2label)"
   ],
   "id": "9d6e2c3a7a192bb1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "train_data, test_data, dev_data = encode_labels_sentiment(fil_train_data, fil_test_data, fil_dev_data, label2id)",
   "id": "bfde5a65efb1560c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# ro-bert\n",
    "tokenizer = BertTokenizer.from_pretrained(\"dumitrescustefan/bert-base-romanian-uncased-v1\", max_length=512,\n",
    "                                          hidden_dropout_prob=0.4, attention_probs_dropout_prob=0.4)  # here"
   ],
   "id": "49c7fdc382818e44",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from model.BERT import BertWithSentiment\n",
    "\n",
    "device = 'cuda' if cuda.is_available() else 'cpu'\n",
    "model = BertWithSentiment(\"dumitrescustefan/bert-base-romanian-uncased-v1\", num_labels=len(logical_fallacies_subset))\n",
    "\n",
    "# Make model weights contiguous\n",
    "for name, param in model.named_parameters():\n",
    "    if not param.is_contiguous():\n",
    "        param.data = param.data.contiguous()\n",
    "model.to(device)"
   ],
   "id": "943f741051d00684",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "train_encodings = tokenizer(list(train_data['source_article_ro']), padding=True, truncation=True, max_length=1024)\n",
    "test_encodings = tokenizer(list(test_data['source_article_ro']), padding=True, truncation=True, max_length=1024)\n",
    "dev_encodings = tokenizer(list(dev_data['source_article_ro']), padding=True, truncation=True, max_length=1024)"
   ],
   "id": "d873ebdd32097e2a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "train_labels = list(train_data['logical_fallacies_id'])\n",
    "test_labels = list(test_data['logical_fallacies_id'])\n",
    "dev_labels = list(dev_data['logical_fallacies_id'])"
   ],
   "id": "1bd42d1088c20387"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "train_sentiments = list(train_data['sentiment_id'])\n",
    "test_sentiments = list(test_data['sentiment_id'])\n",
    "dev_sentiments = list(dev_data['sentiment_id'])"
   ],
   "id": "b6900522e56db68e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "train_dataloader = DataLoader(train_encodings, train_labels, train_sentiments)\n",
    "test_dataloader = DataLoader(test_encodings, test_labels, test_sentiments)\n",
    "dev_dataloader = DataLoader(dev_encodings, dev_labels, dev_sentiments)"
   ],
   "id": "d9957cc1811f96ca"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "training_args = TrainingArguments(\n",
    "    # The output directory where the model predictions and checkpoints will be written\n",
    "    output_dir=\"./results\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    logging_strategy=\"epoch\",  # Log at the end of each epoch\n",
    "    logging_dir=\"./results/logs\",  # Directory for logs\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=float(config['model']['params']['learning_rate']),\n",
    "    per_device_train_batch_size=config['model']['params']['train_batch_size'],\n",
    "    per_device_eval_batch_size=config['model']['params']['eval_batch_size'],\n",
    "    num_train_epochs=config['model']['params']['epochs'],\n",
    "    weight_decay=0.1,\n",
    "    max_grad_norm=1.0,  # Prevents gradient explosion\n",
    "    log_level=\"warning\",\n",
    "    load_best_model_at_end=True\n",
    ")\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataloader,\n",
    "    eval_dataset=dev_dataloader,\n",
    "    compute_metrics=compute_metrics_wandb,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]  # evaluation loss\n",
    ")"
   ],
   "id": "16c22fbf0e5ddc40"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Start training\n",
    "trainer.train()"
   ],
   "id": "19966d36977592b0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "plot = plot_training_curve(trainer, name + \"loss_acc\" + \".png\")",
   "id": "89d8a5142cdccae0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "plot = plot_learning_curve(trainer, name + \"learning_curve\" + \".png\")",
   "id": "e4558195dba00fa4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "wandb.finish()",
   "id": "e814aa04b9a06f57"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "os.makedirs(\"outputs\", exist_ok=True)\n",
    "m_p = os.path.join(\"outputs\", \"model.pt\")  # file\n",
    "torch.save(model.state_dict(), m_p)"
   ],
   "id": "6ec76e31478ad198"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "tokenizer_path = os.path.join(\"outputs\", \"tokenizer\")  # directory\n",
    "tokenizer.save_pretrained(tokenizer_path)"
   ],
   "id": "5756bd8c8d78f2a3"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
