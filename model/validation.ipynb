{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from transformers import BertForSequenceClassification, BertTokenizerFast, pipeline, BertModel, BertTokenizer\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "from utils.data_utils import read_data\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from model.BERT import BertWithSentiment\n",
    "import torch"
   ],
   "id": "6cc27c6af092ff1d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "train_dataset, test_dataset, dev_dataset = read_data(\"../data/all/combined_lfud_huggingface_nonfallacies.csv\")\n",
    "# train_dataset, test_dataset, dev_dataset = read_data(\"../data/all/combined_lfud_huggingface_binary.csv\")"
   ],
   "id": "9b1deb954bc7a3bb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# model_path = \"../model/outputs/21-02-2025_14-45-55_bert-2-classes-model.pickle\" \n",
    "# model_path = \"../model/outputs/03-03-2025_16-23-08_bert-3-classes-model.pickle\"\n",
    "# model_path = \"../model/outputs/03-03-2025_16-46-39_bert-5-classes-model.pickle\" \n",
    "model_path = \"../model/outputs/20-02-2025_10-26-38_bert-all-classes-model.pickle\"\n",
    "# model_path = \"../model/outputs/29-03-2025_14-38-54_bert-3-classes-model.pickle\"\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained(model_path)\n",
    "tokenizer = BertTokenizerFast.from_pretrained(model_path)\n",
    "nlp = pipeline(\"text-classification\", model=model, tokenizer=tokenizer)"
   ],
   "id": "eb04da53dfe45944",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# logical_fallacies = ['nonfallacy', 'fallacy']\n",
    "# logical_fallacies = ['nonfallacy', 'faulty generalization', 'intentional']\n",
    "# logical_fallacies = ['nonfallacy', 'faulty generalization', 'intentional', 'ad hominem', 'false causality']\n",
    "logical_fallacies = list(set(list(test_dataset['logical_fallacies'])))\n",
    "filtered_test_data = test_dataset[test_dataset.logical_fallacies.isin(logical_fallacies)]\n",
    "filtered_test_data"
   ],
   "id": "221631a5c0bc223f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "filtered_test_data[\"logical_fallacies\"].value_counts()",
   "id": "99a4d0bdcf1bcddd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "predictions = []\n",
    "for data in filtered_test_data[\"source_article_ro\"]:\n",
    "    predictions.append(nlp(data)[0][\"label\"])"
   ],
   "id": "750186b38c71ecdf",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "predictions = []\n",
    "filtered_test_data = filtered_test_data[filtered_test_data.logical_fallacies.isin(['appeal to emotion'])]\n",
    "for data in filtered_test_data[\"source_article_ro\"]:\n",
    "    predictions.append((nlp(data)[0][\"label\"], data))"
   ],
   "id": "19a51fd0c9d8ea8f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "nlp(\"Această dezbatere – așa cum susțin pe larg în Watermelons – a fost întotdeauna despre ideologia de stânga, isteria cvasi-religioasă și corupția „urmărește banii”, niciodată despre „știință”.\")[\n",
    "    0][\"label\"]"
   ],
   "id": "9ab024e3e87488b7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for i in range(len(predictions)):\n",
    "    if predictions[i][0] != \"appeal to emotion\":\n",
    "        print(predictions[i], i)"
   ],
   "id": "12c13ed9e4293421",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "report = classification_report(filtered_test_data[\"logical_fallacies\"], predictions)\n",
    "accuracy = accuracy_score(filtered_test_data[\"logical_fallacies\"], predictions)\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Classification Report:\\n\", report)"
   ],
   "id": "12e6c68e63c712c1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "y_true = np.array(filtered_test_data[\"logical_fallacies\"])\n",
    "y_pred = np.array(predictions)\n",
    "\n",
    "classes = np.unique(y_true)\n",
    "\n",
    "for cls in classes:\n",
    "    cls_mask = (y_true == cls)\n",
    "    cls_correct = (y_true[cls_mask] == y_pred[cls_mask])\n",
    "    cls_accuracy = cls_correct.sum() / cls_mask.sum()\n",
    "    print(f\"Class {cls} Accuracy: {cls_accuracy:.2f}\")\n"
   ],
   "id": "32aaf2c5bbaa6968",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Confusion Matrix",
   "id": "89bd1490e4bf0178"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class_labels = sorted(logical_fallacies)  # Ensure consistent ordering\n",
    "\n",
    "cm = confusion_matrix(filtered_test_data[\"logical_fallacies\"], predictions, labels=class_labels)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=class_labels, yticklabels=class_labels)\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()\n"
   ],
   "id": "4d43a86b74d1efbf",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Validation - sentiment",
   "id": "9f58c9e5807c9353"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "train_dataset, test_dataset, dev_dataset = read_data(\"../data/all/combined_lfud_huggingface_nonfallacies_sent.csv\",\n",
    "                                                     sentiment=True)\n",
    "\n",
    "# logical_fallacies = ['fallacy', 'nonfallacy']\n",
    "# logical_fallacies = ['nonfallacy', 'faulty generalization', 'intentional']\n",
    "# logical_fallacies = ['nonfallacy', 'faulty generalization', 'intentional', 'ad hominem', 'false causality']\n",
    "logical_fallacies = ['faulty generalization',\n",
    "                     'false dilemma',\n",
    "                     'appeal to emotion',\n",
    "                     'deductive fallacy',\n",
    "                     'fallacy of extension',\n",
    "                     'false causality',\n",
    "                     'fallacy of relevance',\n",
    "                     'intentional',\n",
    "                     'ad hominem',\n",
    "                     'circular reasoning',\n",
    "                     'fallacy of credibility',\n",
    "                     'ad populum',\n",
    "                     'equivocation',\n",
    "                     'nonfallacy',\n",
    "                     'fallacy of logic']\n",
    "\n",
    "filtered_test_data = test_dataset[test_dataset.logical_fallacies.isin(logical_fallacies)]"
   ],
   "id": "b608d3edfacdfef4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "filtered_test_data",
   "id": "ffc0b98a4732bd8a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Load model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_path = \"/content/gdrive/MyDrive/model/outputs/model.pt\"\n",
    "tokenizer_path = \"/content/gdrive/MyDrive/model/outputs/tokenizer\"\n",
    "\n",
    "model_name = \"dumitrescustefan/bert-base-romanian-uncased-v1\"\n",
    "\n",
    "# Get number of labels (you must know your training labels)\n",
    "# logical_fallacies = list(set(list(filtered_test_data['logical_fallacies'])))\n",
    "label2id = {label: id for id, label in enumerate(logical_fallacies)}\n",
    "id2label = {v: k for k, v in label2id.items()}\n",
    "num_labels = len(label2id)\n",
    "\n",
    "# Recreate model and load weights\n",
    "model = BertWithSentiment(model_name=model_name, num_labels=num_labels)\n",
    "model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(tokenizer_path)\n",
    "\n",
    "# Define sentiment mapping\n",
    "sentiment_mapping = {\"negative\": 0, \"neutral\": 1, \"positive\": 2}\n",
    "\n",
    "\n",
    "# Tokenize test data\n",
    "def tokenize_function(texts, sentiments, tokenizer):\n",
    "    inputs = tokenizer(texts, padding=True, truncation=True, max_length=512, return_tensors=\"pt\")\n",
    "    sentiment_ids = torch.tensor([sentiment_mapping[s] for s in sentiments])\n",
    "    return inputs, sentiment_ids\n",
    "\n",
    "\n",
    "# Prepare dataset\n",
    "texts = filtered_test_data[\"source_article_ro\"].tolist()\n",
    "sentiments = filtered_test_data[\"sentiment\"].tolist()\n",
    "labels = filtered_test_data[\"logical_fallacies\"].tolist()  # Optional\n",
    "\n",
    "# Tokenize\n",
    "inputs, sentiment_ids = tokenize_function(texts, sentiments, tokenizer)\n",
    "inputs = {key: val.to(device) for key, val in inputs.items()}\n",
    "sentiment_ids = sentiment_ids.to(device)\n",
    "\n",
    "# Remove token_type_ids\n",
    "if \"token_type_ids\" in inputs:\n",
    "    inputs.pop(\"token_type_ids\")\n"
   ],
   "id": "5472aa18229f79a7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Run inference\n",
    "with torch.no_grad():\n",
    "    outputs = model(input_ids=inputs[\"input_ids\"], attention_mask=inputs[\"attention_mask\"], sentiment=sentiment_ids)\n",
    "    predictions = torch.argmax(outputs[\"logits\"], dim=1)\n",
    "\n",
    "# Convert predictions to labels\n",
    "predicted_labels = [id2label[pred.item()] for pred in predictions]"
   ],
   "id": "d3106941e155e43f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "\n",
    "report = classification_report(filtered_test_data[\"logical_fallacies\"], predicted_labels)\n",
    "accuracy = accuracy_score(filtered_test_data[\"logical_fallacies\"], predicted_labels)\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Classification Report:\\n\", report)"
   ],
   "id": "597730dac5ace209",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "\n",
    "y_true = np.array(filtered_test_data[\"logical_fallacies\"])\n",
    "y_pred = np.array(predicted_labels)\n",
    "\n",
    "classes = np.unique(y_true)\n",
    "\n",
    "for cls in classes:\n",
    "    cls_mask = (y_true == cls)\n",
    "    cls_correct = (y_true[cls_mask] == y_pred[cls_mask])\n",
    "    cls_accuracy = cls_correct.sum() / cls_mask.sum()\n",
    "    print(f\"Class {cls} Accuracy: {cls_accuracy:.2f}\")\n"
   ],
   "id": "e75ad8c6cc44067f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class_labels = sorted(logical_fallacies)  # Ensure consistent ordering\n",
    "\n",
    "cm = confusion_matrix(filtered_test_data[\"logical_fallacies\"], predicted_labels, labels=class_labels)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=class_labels, yticklabels=class_labels)\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()"
   ],
   "id": "146bd4195b98882b",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
