{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-04-26T08:13:30.751116Z",
     "start_time": "2025-04-26T08:13:14.513432Z"
    }
   },
   "source": [
    "from transformers import BertForSequenceClassification, BertTokenizerFast, pipeline\n",
    "from utils.data_utils import read_data\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "# train_dataset, test_dataset, dev_dataset = read_data(\"../data/all/combined_lfud_huggingface_nonfallacies.csv\")\n",
    "train_dataset, test_dataset, dev_dataset = read_data(\"../data/all/combined_lfud_huggingface_binary.csv\")"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# model_path = \"../model/outputs/21-02-2025_14-45-55_bert-2-classes-model.pickle\" \n",
    "# model_path = \"../model/outputs/03-03-2025_16-23-08_bert-3-classes-model.pickle\"\n",
    "# model_path = \"../model/outputs/03-03-2025_16-46-39_bert-5-classes-model.pickle\" \n",
    "# model_path = \"../model/outputs/20-02-2025_10-26-38_bert-all-classes-model.pickle\" \n",
    "model_path = \"../model/outputs/29-03-2025_14-38-54_bert-3-classes-model.pickle\" \n",
    "\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained(model_path)\n",
    "tokenizer = BertTokenizerFast.from_pretrained(model_path)\n",
    "nlp = pipeline(\"text-classification\", model=model, tokenizer=tokenizer)"
   ],
   "id": "eb04da53dfe45944",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# logical_fallacies = ['nonfallacy', 'fallacy']\n",
    "logical_fallacies = ['nonfallacy', 'faulty generalization', 'intentional']\n",
    "# logical_fallacies = ['nonfallacy', 'faulty generalization', 'intentional', 'ad hominem', 'false causality']\n",
    "# logical_fallacies = list(set(list(test_dataset['logical_fallacies'])))\n",
    "filtered_test_data = test_dataset[test_dataset.logical_fallacies.isin(logical_fallacies)]\n",
    "filtered_test_data"
   ],
   "id": "221631a5c0bc223f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "9edf5fd602a58b9f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "filtered_test_data[\"logical_fallacies\"].value_counts()",
   "id": "99a4d0bdcf1bcddd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "predictions = []\n",
    "for data in filtered_test_data[\"source_article_ro\"]:\n",
    "    predictions.append(nlp(data)[0][\"label\"])"
   ],
   "id": "750186b38c71ecdf",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-26T13:26:12.294607Z",
     "start_time": "2025-04-26T13:26:11.169499Z"
    }
   },
   "cell_type": "code",
   "source": [
    "report = classification_report(filtered_test_data[\"logical_fallacies\"], predictions)\n",
    "accuracy = accuracy_score(filtered_test_data[\"logical_fallacies\"], predictions)\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Classification Report:\\n\", report)"
   ],
   "id": "12e6c68e63c712c1",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'filtered_test_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[2], line 2\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01msklearn\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mmetrics\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m classification_report, accuracy_score, confusion_matrix\n\u001B[1;32m----> 2\u001B[0m report \u001B[38;5;241m=\u001B[39m classification_report(\u001B[43mfiltered_test_data\u001B[49m[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlogical_fallacies\u001B[39m\u001B[38;5;124m\"\u001B[39m], predictions)\n\u001B[0;32m      3\u001B[0m accuracy \u001B[38;5;241m=\u001B[39m accuracy_score(filtered_test_data[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlogical_fallacies\u001B[39m\u001B[38;5;124m\"\u001B[39m], predictions)\n\u001B[0;32m      5\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAccuracy:\u001B[39m\u001B[38;5;124m\"\u001B[39m, accuracy)\n",
      "\u001B[1;31mNameError\u001B[0m: name 'filtered_test_data' is not defined"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "\n",
    "y_true = np.array(filtered_test_data[\"logical_fallacies\"])\n",
    "y_pred = np.array(predictions)\n",
    "\n",
    "classes = np.unique(y_true) \n",
    "\n",
    "for cls in classes:\n",
    "    cls_mask = (y_true == cls)\n",
    "    cls_correct = (y_true[cls_mask] == y_pred[cls_mask])\n",
    "    cls_accuracy = cls_correct.sum() / cls_mask.sum()\n",
    "    print(f\"Class {cls} Accuracy: {cls_accuracy:.2f}\")\n"
   ],
   "id": "32aaf2c5bbaa6968",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Confusion Matrix",
   "id": "89bd1490e4bf0178"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class_labels = sorted(logical_fallacies)  # Ensure consistent ordering\n",
    "\n",
    "cm = confusion_matrix(filtered_test_data[\"logical_fallacies\"], predictions, labels=class_labels)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=class_labels, yticklabels=class_labels)\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()\n"
   ],
   "id": "4d43a86b74d1efbf",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Validation - sentiment",
   "id": "9f58c9e5807c9353"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-26T13:41:51.156838Z",
     "start_time": "2025-04-26T13:41:51.072923Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from utils.data_utils import read_data\n",
    "\n",
    "train_dataset, test_dataset, dev_dataset = read_data(\"../data/all/combined_lfud_huggingface_nonfallacies_sent.csv\", sentiment = True)\n",
    "\n",
    "# logical_fallacies = ['nonfallacy', 'fallacy']\n",
    "# logical_fallacies = ['nonfallacy', 'faulty generalization', 'intentional']\n",
    "# logical_fallacies = ['nonfallacy', 'faulty generalization', 'intentional', 'ad hominem', 'false causality']\n",
    "logical_fallacies = list(set(list(train_dataset['logical_fallacies'])))\n",
    "\n",
    "filtered_test_data = test_dataset[test_dataset.logical_fallacies.isin(logical_fallacies)]"
   ],
   "id": "b608d3edfacdfef4",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-26T13:41:52.748847Z",
     "start_time": "2025-04-26T13:41:52.723362Z"
    }
   },
   "cell_type": "code",
   "source": "filtered_test_data",
   "id": "ffc0b98a4732bd8a",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "           logical_fallacies  \\\n",
       "4279           false dilemma   \n",
       "4280   faulty generalization   \n",
       "4281         false causality   \n",
       "4282        fallacy of logic   \n",
       "4283  fallacy of credibility   \n",
       "...                      ...   \n",
       "5129    fallacy of relevance   \n",
       "5130       appeal to emotion   \n",
       "5131             intentional   \n",
       "5132   faulty generalization   \n",
       "5133           false dilemma   \n",
       "\n",
       "                                      source_article_ro sentiment  \n",
       "4279             Dacă este știință, nu este un consens.   neutral  \n",
       "4280  „Dacă nu obții un A la clasa domnului K, vei p...  negative  \n",
       "4281  De fiecare dată când port acest colier, îmi pr...   neutral  \n",
       "4282  Forarea carotelor de gheață arată că la 800 de...  positive  \n",
       "4283  Nu este timpul să începem să ignorăm afirmații...  negative  \n",
       "...                                                 ...       ...  \n",
       "5129  O eroare logică care compară abaterile minore ...  negative  \n",
       "5130                Este deja aici. Și se va înrăutăți.  negative  \n",
       "5131  Poate cercetarea climatică să revină pe drumul...  negative  \n",
       "5132  Toate cursurile universitare necesare sunt pli...  negative  \n",
       "5133  Fie interzicem adolescenților să dețină telefo...  negative  \n",
       "\n",
       "[855 rows x 3 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>logical_fallacies</th>\n",
       "      <th>source_article_ro</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4279</th>\n",
       "      <td>false dilemma</td>\n",
       "      <td>Dacă este știință, nu este un consens.</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4280</th>\n",
       "      <td>faulty generalization</td>\n",
       "      <td>„Dacă nu obții un A la clasa domnului K, vei p...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4281</th>\n",
       "      <td>false causality</td>\n",
       "      <td>De fiecare dată când port acest colier, îmi pr...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4282</th>\n",
       "      <td>fallacy of logic</td>\n",
       "      <td>Forarea carotelor de gheață arată că la 800 de...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4283</th>\n",
       "      <td>fallacy of credibility</td>\n",
       "      <td>Nu este timpul să începem să ignorăm afirmații...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5129</th>\n",
       "      <td>fallacy of relevance</td>\n",
       "      <td>O eroare logică care compară abaterile minore ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5130</th>\n",
       "      <td>appeal to emotion</td>\n",
       "      <td>Este deja aici. Și se va înrăutăți.</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5131</th>\n",
       "      <td>intentional</td>\n",
       "      <td>Poate cercetarea climatică să revină pe drumul...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5132</th>\n",
       "      <td>faulty generalization</td>\n",
       "      <td>Toate cursurile universitare necesare sunt pli...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5133</th>\n",
       "      <td>false dilemma</td>\n",
       "      <td>Fie interzicem adolescenților să dețină telefo...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>855 rows × 3 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-26T13:41:56.007573Z",
     "start_time": "2025-04-26T13:41:55.996628Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import BertModel\n",
    "import torch.nn as nn\n",
    "\n",
    "sentiment_mapping = {\"negative\": 0, \"neutral\": 1, \"positive\": 2}\n",
    "\n",
    "class BertWithSentiment(nn.Module):\n",
    "    def __init__(self, model_name, num_labels, num_sentiment_classes=3):\n",
    "        super(BertWithSentiment, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained(model_name, return_dict=True)  # Ensure return_dict=True\n",
    "        self.sentiment_embedding = nn.Embedding(num_sentiment_classes, 768)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.classifier = nn.Linear(768 * 2, num_labels)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, sentiment, labels=None):\n",
    "        bert_output = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        cls_token_embedding = bert_output.last_hidden_state[:, 0, :]\n",
    "        sentiment_embed = self.sentiment_embedding(sentiment)\n",
    "        combined = torch.cat((cls_token_embedding, sentiment_embed), dim=1)\n",
    "\n",
    "        logits = self.classifier(self.dropout(combined))\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "            loss = loss_fct(logits, labels)  # Ensure labels are (batch_size,) with class indices\n",
    "\n",
    "        return {\"loss\": loss, \"logits\": logits} if loss is not None else {\"logits\": logits}"
   ],
   "id": "b64b4f0a085ae73b",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-04-26T13:41:56.785760Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "# Load model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_path = \"../model/outputs/outputs/model.pt\"\n",
    "tokenizer_path = \"../model/outputs/outputs/tokenizer\"\n",
    "\n",
    "model_name = \"dumitrescustefan/bert-base-romanian-uncased-v1\"\n",
    "\n",
    "# Get number of labels (you must know your training labels)\n",
    "# logical_fallacies = list(set(list(filtered_test_data['logical_fallacies'])))\n",
    "label2id = {label: id for id, label in enumerate(logical_fallacies)}\n",
    "id2label = {v: k for k, v in label2id.items()}\n",
    "num_labels = len(label2id)\n",
    "\n",
    "# Recreate model and load weights\n",
    "model = BertWithSentiment(model_name=model_name, num_labels=num_labels)\n",
    "model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(tokenizer_path)\n",
    "\n",
    "# Define sentiment mapping\n",
    "sentiment_mapping = {\"negative\": 0, \"neutral\": 1, \"positive\": 2}\n",
    "\n",
    "# Tokenize test data\n",
    "def tokenize_function(texts, sentiments, tokenizer):\n",
    "    inputs = tokenizer(texts, padding=True, truncation=True, max_length=512, return_tensors=\"pt\")\n",
    "    sentiment_ids = torch.tensor([sentiment_mapping[s] for s in sentiments])\n",
    "    return inputs, sentiment_ids\n",
    "\n",
    "# Prepare dataset\n",
    "texts = filtered_test_data[\"source_article_ro\"].tolist()\n",
    "sentiments = filtered_test_data[\"sentiment\"].tolist()\n",
    "labels = filtered_test_data[\"logical_fallacies\"].tolist()  # Optional\n",
    "\n",
    "# Tokenize\n",
    "inputs, sentiment_ids = tokenize_function(texts, sentiments, tokenizer)\n",
    "inputs = {key: val.to(device) for key, val in inputs.items()}\n",
    "sentiment_ids = sentiment_ids.to(device)\n",
    "\n",
    "# Remove token_type_ids\n",
    "if \"token_type_ids\" in inputs:\n",
    "    inputs.pop(\"token_type_ids\")\n",
    "\n",
    "# Run inference\n",
    "with torch.no_grad():\n",
    "    outputs = model(input_ids=inputs[\"input_ids\"], attention_mask=inputs[\"attention_mask\"], sentiment=sentiment_ids)\n",
    "    predictions = torch.argmax(outputs[\"logits\"], dim=1)\n",
    "\n",
    "# Convert predictions to labels\n",
    "predicted_labels = [id2label[pred.item()] for pred in predictions]\n",
    "\n",
    "predicted_labels"
   ],
   "id": "5472aa18229f79a7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "\n",
    "report = classification_report(filtered_test_data[\"logical_fallacies\"], predicted_labels)\n",
    "accuracy = accuracy_score(filtered_test_data[\"logical_fallacies\"], predicted_labels)\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Classification Report:\\n\", report)"
   ],
   "id": "597730dac5ace209",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "\n",
    "y_true = np.array(filtered_test_data[\"logical_fallacies\"])\n",
    "y_pred = np.array(predicted_labels)\n",
    "\n",
    "classes = np.unique(y_true)\n",
    "\n",
    "for cls in classes:\n",
    "    cls_mask = (y_true == cls)\n",
    "    cls_correct = (y_true[cls_mask] == y_pred[cls_mask])\n",
    "    cls_accuracy = cls_correct.sum() / cls_mask.sum()\n",
    "    print(f\"Class {cls} Accuracy: {cls_accuracy:.2f}\")\n"
   ],
   "id": "e75ad8c6cc44067f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "class_labels = sorted(logical_fallacies)  # Ensure consistent ordering\n",
    "\n",
    "cm = confusion_matrix(filtered_test_data[\"logical_fallacies\"], predicted_labels, labels=class_labels)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=class_labels, yticklabels=class_labels)\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()"
   ],
   "id": "146bd4195b98882b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "2982ed971a56787c"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
