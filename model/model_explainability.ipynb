{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Only BERT",
   "id": "2719f4bdcf3d424f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import shap\n",
    "import torch\n",
    "import numpy as np\n",
    "from model.BERT import BertWithSentiment\n",
    "from utils.data_utils import read_data\n",
    "from transformers import pipeline, BertForSequenceClassification, BertTokenizerFast, BertTokenizer, BertModel"
   ],
   "id": "11108b0af984b172"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def explain_bert(model_path, test_dataset, logical_fallacies):\n",
    "    \"\"\"\n",
    "    Generates SHAP (SHapley Additive exPlanations) visualizations to explain predictions made by a \n",
    "    fine-tuned BERT model on a text classification task.\n",
    "\n",
    "    The function filters the test dataset by a subset of logical fallacy labels, loads the specified \n",
    "    BERT model and tokenizer, and uses SHAP to compute and visualize the contribution of each word \n",
    "    in the input texts to the model's predictions.\n",
    "\n",
    "    Args:\n",
    "        model_path (str): Path or model name of the fine-tuned BERT model and tokenizer directory \n",
    "            (e.g., Hugging Face model hub ID or local directory).\n",
    "        test_dataset (pd.DataFrame): A pandas DataFrame containing the test data, including a \n",
    "            column with texts and a 'logical_fallacies' column for labels.\n",
    "        logical_fallacies (list of str): A list of logical fallacy class labels to filter the \n",
    "            test dataset by (e.g., ['ad hominem', 'straw man']).\n",
    "\n",
    "    Returns:\n",
    "        None: Displays SHAP text plots in a browser or notebook, highlighting which words influenced \n",
    "        the model's predictions.\n",
    "    \"\"\"\n",
    "    filtered_test_data = test_dataset[test_dataset.logical_fallacies.isin(logical_fallacies)]\n",
    "\n",
    "    # Load model\n",
    "    model = BertForSequenceClassification.from_pretrained(model_path)\n",
    "    tokenizer = BertTokenizerFast.from_pretrained(model_path)\n",
    "    nlp = pipeline(\"text-classification\", model=model, tokenizer=tokenizer, top_k=None)\n",
    "\n",
    "    # Predict function for SHAP\n",
    "    def predict_proba(texts):\n",
    "        if isinstance(texts, np.ndarray):\n",
    "            texts = texts.tolist()\n",
    "        preds = nlp(texts)\n",
    "        return np.array([[item['score'] for item in pred] for pred in preds])\n",
    "\n",
    "    # Create SHAP explainer\n",
    "    explainer = shap.Explainer(predict_proba, masker=shap.maskers.Text(tokenizer))\n",
    "\n",
    "    # Select a few examples\n",
    "    sample_texts = filtered_test_data[\"source_article_ro\"].sample(10, random_state=42).tolist()\n",
    "\n",
    "    # Compute SHAP values\n",
    "    shap_values = explainer(sample_texts)\n",
    "\n",
    "    # Plot explanations\n",
    "    shap.plots.text(shap_values)\n"
   ],
   "id": "33dfe300a2e86b3d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "_, test_dataset, _ = read_data(\"../data/all/combined_lfud_huggingface_binary.csv\")",
   "id": "5ad2149ce74e6882",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "model_path = \"../model/outputs/21-02-2025_14-45-55_bert-2-classes-model.pickle\"\n",
    "logical_fallacies = ['nonfallacy', 'fallacy']\n",
    "explain_bert(model_path, test_dataset, logical_fallacies)"
   ],
   "id": "703317ae97862a6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "_, test_dataset, _ = read_data(\"../data/all/combined_lfud_huggingface_nonfallacies.csv\")",
   "id": "b00bd46d4738a0c6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "model_path = \"../model/outputs/03-03-2025_16-23-08_bert-3-classes-model.pickle\"\n",
    "logical_fallacies = ['nonfallacy', 'faulty generalization', 'intentional']\n",
    "explain_bert(model_path, test_dataset, logical_fallacies)"
   ],
   "id": "22fa4b778e649cc5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "model_path = \"../model/outputs/03-03-2025_16-46-39_bert-5-classes-model.pickle\"\n",
    "logical_fallacies = ['nonfallacy', 'faulty generalization', 'intentional', 'ad hominem', 'false causality']\n",
    "explain_bert(model_path, test_dataset, logical_fallacies)"
   ],
   "id": "f8b3ca846a3c51a4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "model_path = \"../model/outputs/20-02-2025_10-26-38_bert-all-classes-model.pickle\"\n",
    "logical_fallacies = list(set(list(test_dataset['logical_fallacies'])))\n",
    "explain_bert(model_path, test_dataset, logical_fallacies)"
   ],
   "id": "1b4b1072f1a07572",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### BERT with sentiment",
   "id": "1a19f00ac8719a62"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class FixedSentimentMasker:\n",
    "    def __init__(self, tokenizer, sentiments):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.sentiments = sentiments  # store sentiments\n",
    "\n",
    "    def __call__(self, inputs):\n",
    "        # inputs is a list of masked texts\n",
    "        masked_texts = np.array(inputs)  # wrap texts in np.array\n",
    "        metadata = np.zeros(len(inputs))  # dummy metadata\n",
    "        return masked_texts, metadata\n"
   ],
   "id": "d7f795b0c1cd12bd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "model_name = \"dumitrescustefan/bert-base-romanian-uncased-v1\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "def explain_bert_with_sentiment(model_path, tokenizer_path, test_dataset, logical_fallacies, fig_file, fallacy_index):\n",
    "    \"\"\"\n",
    "    Explains predictions of a BERT-based classifier augmented with sentiment embeddings using SHAP,\n",
    "    and visualizes the influence of sentiment on the model's output.\n",
    "\n",
    "    The function filters the test dataset by logical fallacy labels, loads the `BertWithSentiment` model\n",
    "    and tokenizer, computes SHAP explanations for input texts, and analyzes the effect of sentiment.\n",
    "\n",
    "    Args:\n",
    "        model_path (str): Path to the saved fine-tuned `BertWithSentiment` model weights.\n",
    "        tokenizer_path (str): Path to the tokenizer used with the model.\n",
    "        test_dataset (pd.DataFrame): Test data containing columns 'source_article_ro', 'logical_fallacies', and 'sentiment'.\n",
    "        logical_fallacies (list of str): Logical fallacy labels used to filter the test dataset.\n",
    "        fig_file (str): File path to save the sentiment impact plot.\n",
    "        fallacy_index (int): Index of a test sample within the filtered dataset to analyze in detail.\n",
    "\n",
    "    Returns:\n",
    "        None: Displays SHAP text plots and a matplotlib plot showing sentiment impact. Prints sentiment embedding norms.\n",
    "    \"\"\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    filtered_test_data = test_dataset[test_dataset.logical_fallacies.isin(logical_fallacies)]\n",
    "\n",
    "    label2id = {label: id for id, label in enumerate(logical_fallacies)}\n",
    "    id2label = {v: k for k, v in label2id.items()}\n",
    "    num_labels = len(label2id)\n",
    "\n",
    "    model = BertWithSentiment(model_name=model_name, num_labels=num_labels)\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    tokenizer = BertTokenizer.from_pretrained(tokenizer_path)\n",
    "\n",
    "    sentiment_mapping = {\"negative\": 0, \"neutral\": 1, \"positive\": 2}\n",
    "\n",
    "    # Define prediction function\n",
    "    def predict_proba(texts):\n",
    "        \"\"\"\n",
    "        Predicts class probabilities for a batch of texts with associated sentiments.\n",
    "\n",
    "        Args:\n",
    "            texts (list of str or list of (str, str)): List of input texts or tuples of (text, sentiment_label).\n",
    "                If sentiment is not provided, 'neutral' is assumed.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: Array of shape (batch_size, num_classes) with predicted probabilities.\n",
    "        \"\"\"\n",
    "        # texts is a list of strings normally, but SHAP can pass in masked text\n",
    "        if isinstance(texts[0], tuple):\n",
    "            texts, sentiments = zip(*texts)\n",
    "        else:\n",
    "            sentiments = [\"neutral\"] * len(texts)  # fallback if no sentiment given\n",
    "\n",
    "        inputs = tokenizer(list(texts), padding=True, truncation=True, max_length=512, return_tensors=\"pt\")\n",
    "        sentiment_ids = torch.tensor([sentiment_mapping.get(s, 1) for s in sentiments])\n",
    "\n",
    "        inputs = {key: val.to(device) for key, val in inputs.items()}\n",
    "        sentiment_ids = sentiment_ids.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids=inputs[\"input_ids\"], attention_mask=inputs[\"attention_mask\"],\n",
    "                            sentiment=sentiment_ids)\n",
    "            logits = outputs[\"logits\"]\n",
    "            probs = torch.softmax(logits, dim=-1).cpu().numpy()\n",
    "\n",
    "        return probs\n",
    "\n",
    "    def explain_sentiment_impact(text, sentiments=[\"negative\", \"neutral\", \"positive\"]):\n",
    "        \"\"\"\n",
    "        Plots how changing the sentiment input(perturbations) affects the model's predicted class probabilities for a given text.\n",
    "\n",
    "        Args:\n",
    "            text (str): The input text to analyze.\n",
    "            sentiments (list of str, optional): List of sentiment labels to test. Defaults to [\"negative\", \"neutral\", \"positive\"].\n",
    "\n",
    "        Returns:\n",
    "            None: Shows and saves a line plot illustrating probability changes across sentiments.\n",
    "        \"\"\"\n",
    "        import matplotlib.pyplot as plt\n",
    "        import numpy as np\n",
    "\n",
    "        inputs = [(text, s) for s in sentiments]\n",
    "        probs = predict_proba(inputs)\n",
    "\n",
    "        probs = np.array(probs)\n",
    "\n",
    "        # Print and store predictions as annotations\n",
    "        for s, p in zip(sentiments, probs):\n",
    "            print(f\"Sentiment: {s} => Prediction: {p}\")\n",
    "\n",
    "        # Create the figure and axis\n",
    "        fig, ax = plt.subplots()\n",
    "\n",
    "        # Plot impact\n",
    "        for i in range(probs.shape[1]):\n",
    "            ax.plot(sentiments, probs[:, i], label=f'Class {i}', marker='o')\n",
    "\n",
    "        ax.set_title(\"Effect of Sentiment on Prediction\")\n",
    "        ax.set_ylabel(\"Probability\")\n",
    "        ax.set_xlabel(\"Sentiment\")\n",
    "\n",
    "        # Place legend outside the plot on the right\n",
    "        ax.legend(loc='center left', bbox_to_anchor=(1.02, 0.5), borderaxespad=0)\n",
    "\n",
    "        # Annotate predictions on the plot\n",
    "        for i, sentiment in enumerate(sentiments):\n",
    "            for j in range(probs.shape[1]):\n",
    "                ax.text(i, probs[i, j] + 0.02, f\"{probs[i, j]:.2f}\",\n",
    "                        ha='center', va='bottom', fontsize=8)\n",
    "\n",
    "        # Adjust layout to accommodate the legend\n",
    "        plt.tight_layout(rect=[0, 0, 0.85, 1])  # leave space on the right\n",
    "\n",
    "        # Save the figure\n",
    "        fig_file = \"sentiment_impact.png\"  # Make sure fig_file is defined\n",
    "        plt.savefig(fig_file, bbox_inches='tight')\n",
    "        plt.show()\n",
    "\n",
    "    def get_sentiment_contribution(text, sentiment_label):\n",
    "        \"\"\"\n",
    "        Extracts intermediate embeddings from the model to isolate the contribution of the sentiment embedding.\n",
    "\n",
    "        Args:\n",
    "            text (str): Input text.\n",
    "            sentiment_label (str): Sentiment label whose embedding contribution is extracted.\n",
    "\n",
    "        Returns:\n",
    "            tuple:\n",
    "                - cls_embedding (np.ndarray): The BERT CLS token embedding (shape: 1 x hidden_size).\n",
    "                - sentiment_embedding (np.ndarray): The sentiment embedding vector (shape: 1 x hidden_size).\n",
    "        \"\"\"\n",
    "        # Extract intermediate values to isolate the effect of sentiment\n",
    "        inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "        sentiment_id = torch.tensor([sentiment_mapping[sentiment_label]]).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            bert_output = model.bert(**inputs)\n",
    "            cls_embedding = bert_output.last_hidden_state[:, 0, :]\n",
    "            sentiment_embed = model.sentiment_embedding(sentiment_id)\n",
    "            return cls_embedding.cpu().numpy(), sentiment_embed.cpu().numpy()\n",
    "\n",
    "    sample = filtered_test_data[:6]\n",
    "    sample_texts = sample[\"source_article_ro\"].tolist()\n",
    "    sample_sentiments = sample[\"sentiment\"].tolist()\n",
    "\n",
    "    explainer = shap.Explainer(predict_proba, masker=shap.maskers.Text(tokenizer))\n",
    "    shap_values = explainer(sample_texts[:5])  # or however many you want\n",
    "\n",
    "    shap.plots.text(shap_values)\n",
    "\n",
    "    text, sentiment = sample_texts[fallacy_index], sample_sentiments[fallacy_index]\n",
    "    _, sentiment_emb = get_sentiment_contribution(text, sentiment)\n",
    "    print(f\"Sentiment embedding L2 norm: {np.linalg.norm(sentiment_emb):.4f}\")\n",
    "\n",
    "    text = sample_texts[fallacy_index]\n",
    "\n",
    "    # SHAP for text\n",
    "    shap_val = explainer([text])\n",
    "    shap.plots.text(shap_val)\n",
    "\n",
    "    # Sentiment impact\n",
    "    explain_sentiment_impact(text)"
   ],
   "id": "147d388d25aaa851",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "_, test_dataset, _ = read_data(\"/kaggle/input/licenta-dataset-model/all/combined_lfud_huggingface_binary_sent.csv\",\n",
    "                               sentiment=True)"
   ],
   "id": "193e08b9345de61e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "model_path = \"/kaggle/input/licenta-dataset-model/outputs/experiment-3_4_sent_2_classes/outputs/model.pt\"\n",
    "tokenizer_path = \"/kaggle/input/licenta-dataset-model/outputs/experiment-3_4_sent_2_classes/outputs/tokenizer\"\n",
    "\n",
    "logical_fallacies = ['nonfallacy', 'fallacy']\n",
    "explain_bert_with_sentiment(model_path, tokenizer_path, test_dataset, logical_fallacies, \"sentiment_impact2.png\", 4)"
   ],
   "id": "e28f90589e07e284",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
